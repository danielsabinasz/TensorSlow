{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning From Scratch: Theory and Implementation\n",
    "\n",
    "In this text, we develop the mathematical and algorithmic underpinnings of deep neural networks from scratch and implement our own neural network library in Python, mimicking the <a href=\"http://www.tensorflow.org\">TensorFlow</a> API. I do not assume that you have any preknowledge about machine learning or neural networks. However, you should have some preknowledge of calculus, linear algebra, fundamental algorithms and probability theory on an undergraduate level. If you get stuck at some point, please leave a comment.\n",
    "\n",
    "By the end of this text, you will have a deep understanding of the math behind neural networks and how deep learning libraries work under the hood.\n",
    "\n",
    "I have tried to keep the code as simple and concise as possible, favoring conceptual clarity over efficiency. Since our API mimicks the TensorFlow API, you will know how to use TensorFlow once you have finished this text, and you will know how TensorFlow works under the hood conceptually (without all the overhead that comes with an omnipotent, maximally efficient machine learning API)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational graphs\n",
    "We shall start by defining the concept of a computational graph, since neural networks are a special form thereof. A computational graph is a directed graph where the nodes correspond to **operations** or **variables**. Variables can feed their value into operations, and operations can feed their output into other operations. This way, every node in the graph defines a function of the variables.\n",
    "\n",
    "The values that are fed into the nodes and come out of the nodes are called <b>tensors</b>, which is just a fancy word for a multi-dimensional array. Hence, it subsumes scalars, vectors and matrices as well as tensors of a higher rank.\n",
    "\n",
    "Let's look at an example. The following computational graph computes the sum $z$ of two inputs $x$ and $y$. \n",
    "Here, $x$ and $y$ are input nodes to $z$ and $z$ is a consumer of $x$ and $y$. $z$ therefore defines a function $z : \\mathbb{R^2} \\rightarrow \\mathbb{R}$ where $z(x, y) = x + y$.\n",
    "\n",
    "<img src=\"addition.png?456\" style=\"height: 200px;\">\n",
    "\n",
    "The concept of a computational graph becomes more useful once the computations become more complex. For example, the following computational graph defines an affine transformation $z(A, x, b) = Ax + b$.\n",
    "\n",
    "<img src=\"affine_transformation.png\" style=\"height: 200px;\">\n",
    "\n",
    "## Operations\n",
    "\n",
    "Every operation is characterized by three things:\n",
    "- A `compute` function that computes the operation's output given values for the operation's inputs\n",
    "- A list of `input_nodes` which can be variables or other operations\n",
    "- A list of `consumers` that use the operation's output as their input\n",
    "\n",
    "Let's put this into code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Operation:\n",
    "    \"\"\"Represents a graph node that performs a computation.\n",
    "    \n",
    "    An `Operation` is a node in a `Graph` that takes zero or\n",
    "    more objects as input, and produces zero or more objects\n",
    "    as output.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_nodes = []):\n",
    "        \"\"\"Construct Operation\n",
    "        \"\"\"\n",
    "        self.input_nodes = input_nodes\n",
    "        \n",
    "        # Initialize list of consumers (i.e. nodes that receive this operation's output as input)\n",
    "        self.consumers = []\n",
    "        \n",
    "        # Append this operation to the list of consumers of all input nodes\n",
    "        for input_node in input_nodes:\n",
    "            input_node.consumers.append(self)\n",
    "        \n",
    "        # Append this operation to the list of operations in the currently active default graph\n",
    "        _default_graph.operations.append(self)\n",
    "  \n",
    "    def compute(self):\n",
    "        \"\"\"Computes the output of this operation.\n",
    "        \"\" Must be implemented by the particular operation.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some elementary operations\n",
    "\n",
    "Let's implement some elementary operations in order to become familiar with the `Operation` class (and because we will need them later).\n",
    "\n",
    "#### Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class add(Operation):\n",
    "    \"\"\"Returns x + y element-wise.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        \"\"\"Construct add\n",
    "        \n",
    "        Args:\n",
    "          x: First summand node\n",
    "          y: Second summand node\n",
    "        \"\"\"\n",
    "        super().__init__([x, y])\n",
    "\n",
    "    def compute(self, x_value, y_value):\n",
    "        \"\"\"Compute the output of the add operation\n",
    "        \n",
    "        Args:\n",
    "          x_value: First summand value\n",
    "          y_value: Second summand value\n",
    "        \"\"\"\n",
    "        self.inputs = [x_value, y_value]\n",
    "        return x_value + y_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class matmul(Operation):\n",
    "    \"\"\"Multiplies matrix a by matrix b, producing a * b.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, a, b):\n",
    "        \"\"\"Construct matmul\n",
    "        \n",
    "        Args:\n",
    "          a: First matrix\n",
    "          b: Second matrix\n",
    "        \"\"\"\n",
    "        super().__init__([a, b])\n",
    "    \n",
    "    def compute(self, a_value, b_value):\n",
    "        \"\"\"Compute the output of the matmul operation\n",
    "        \n",
    "        Args:\n",
    "          a_value: First matrix value\n",
    "          b_value: Second matrix value\n",
    "        \"\"\"\n",
    "        self.inputs = [a_value, b_value]\n",
    "        return a_value.dot(b_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholders\n",
    "\n",
    "Not all the nodes in a computational graph are operations. For example, in the affine transformation graph, $A$, $x$ and $b$ are not operations. Rather, they are inputs to the graph that have to be supplied with a value once we want to compute the output of the graph. To provide such values, we introduce **placeholders**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class placeholder:\n",
    "    \"\"\"Represents a placeholder node that has to be provided with a value\n",
    "       when computing the output of a computational graph\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Construct placeholder\n",
    "        \"\"\"\n",
    "        self.consumers = []\n",
    "        \n",
    "        # Append this placeholder to the list of placeholders in the currently active default graph\n",
    "        _default_graph.placeholders.append(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "In the affine transformation graph, there is a qualitative difference between $x$ on the one hand and $A$ and $b$ on the other hand. While $x$ is an input to the operation, $A$ and $b$ are <b>parameters</b> of the operation, i.e. they are intrinsic to the graph. We will refer to such parameters as <b>Variables</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Variable:\n",
    "    \"\"\"Represents a variable (i.e. an intrinsic, changeable parameter of a computational graph).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, initial_value = None):\n",
    "        \"\"\"Construct Variable\n",
    "        \n",
    "        Args:\n",
    "          initial_value: The initial value of this variable\n",
    "        \"\"\"\n",
    "        self.value = initial_value\n",
    "        self.consumers = []\n",
    "        \n",
    "        # Append this variable to the list of variables in the currently active default graph\n",
    "        _default_graph.variables.append(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Graph class\n",
    "\n",
    "Finally, we'll need a class that bundles all the operations, placeholders and variables together. When creating a new graph, we can call its `as_default` method to set the `_default_graph` to this graph. This way, we can create operations, placeholders and variables without having to pass in a reference to the graph everytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    \"\"\"Represents a computational graph\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Construct Graph\"\"\"\n",
    "        self.operations = []\n",
    "        self.placeholders = []\n",
    "        self.variables = []\n",
    "        \n",
    "    def as_default(self):\n",
    "        global _default_graph\n",
    "        _default_graph = self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Let's now use the classes we have built to create a computational graph for the following affine transformation:\n",
    "\n",
    "$$\n",
    "z = \\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & -1\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "x\n",
    "+\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a new graph\n",
    "Graph().as_default()\n",
    "\n",
    "# Create variables\n",
    "A = Variable([[1, 0], [0, -1]])\n",
    "b = Variable([1, 1])\n",
    "\n",
    "# Create placeholder\n",
    "x = placeholder()\n",
    "\n",
    "# Create hidden node y\n",
    "y = matmul(A, x)\n",
    "\n",
    "# Create output node z\n",
    "z = add(y, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the output of an operation\n",
    "Now that we are confident creating computational graphs, we can start to think about how to compute the output of an operation.\n",
    "\n",
    "Let's create a Session class that encapsulates an execution of an operation. We would like to be able to create a session instance and call a `run` method on this instance, passing the operation that we want to compute and a dictionary containing values for the placeholders:\n",
    "\n",
    "    session = Session()\n",
    "    output = session.run(z, {\n",
    "        x: [1, 2]\n",
    "    })\n",
    "\n",
    "This should compute the following value:\n",
    "\n",
    "$$\n",
    "z = \\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & -1\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "2\n",
    "\\end{pmatrix}\n",
    "+\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "2 \\\\\n",
    "-1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "In order to compute the function represented by an operation, we need to apply the computations in the right order. For example, we cannot compute $z$ before we have computed $y$ as an intermediate result. Therefore, we have to make sure that the operations are carried out in the right order, such that the values of every node that is an input to an operation $o$ has been computed before $o$ is computed. This can be achieved via <a href=\"https://en.wikipedia.org/wiki/Tree_traversal#Post-order\" target=\"new\">post-order traversal</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Session:\n",
    "    \"\"\"Represents a particular execution of a computational graph.\n",
    "    \"\"\"\n",
    "    \n",
    "    def run(self, operation, feed_dict = {}):\n",
    "        \"\"\"Computes the output of an operation\n",
    "        \n",
    "        Args:\n",
    "          operation: The operation whose output we'd like to compute.\n",
    "          feed_dict: A dictionary that maps placeholders to values for this session\n",
    "        \"\"\"\n",
    "        \n",
    "        # Perform a post-order traversal of the graph to bring the nodes into the right order\n",
    "        nodes_postorder = traverse_postorder(operation)\n",
    "        \n",
    "        # Iterate all nodes to determine their value\n",
    "        for node in nodes_postorder:\n",
    "\n",
    "            if type(node) == placeholder:\n",
    "                # Set the node value to the placeholder value from feed_dict\n",
    "                node.output = feed_dict[node]\n",
    "            elif type(node) == Variable:\n",
    "                # Set the node value to the variable's value attribute\n",
    "                node.output = node.value\n",
    "            else: # Operation\n",
    "                # Get the input values for this operation from node_values\n",
    "                node.inputs = [input_node.output for input_node in node.input_nodes]\n",
    "\n",
    "                # Compute the output of this operation\n",
    "                node.output = node.compute(*node.inputs)\n",
    "                \n",
    "            # Convert lists to numpy arrays\n",
    "            if type(node.output) == list:\n",
    "                node.output = np.array(node.output)\n",
    "        \n",
    "        # Return the requested node value\n",
    "        return operation.output\n",
    "\n",
    "\n",
    "def traverse_postorder(operation):\n",
    "    \"\"\"Performs a post-order traversal, returning a list of nodes\n",
    "    in the order in which they have to be computed\n",
    "    \n",
    "    Args:\n",
    "       operation: The operation to start traversal at\n",
    "    \"\"\"\n",
    "    \n",
    "    nodes_postorder = []\n",
    "    def recurse(node):\n",
    "        if isinstance(node, Operation):\n",
    "            for input_node in node.input_nodes:\n",
    "                recurse(input_node)\n",
    "        nodes_postorder.append(node)\n",
    "\n",
    "    recurse(operation)\n",
    "    return nodes_postorder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our class on the example from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2 -1]\n"
     ]
    }
   ],
   "source": [
    "session = Session()\n",
    "output = session.run(z, {\n",
    "    x: [1, 2]\n",
    "})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptrons\n",
    "\n",
    "## A motivating example\n",
    "\n",
    "Perceptrons are a miniature form of neural network and a basic building block of more complex architectures. Before going into the details, let's motivate them by an example. Assume that we are given a dataset consisting of 100 points in the plane. Half of the points are red and half of the points are blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1173988d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create red points centered at (-2, -2)\n",
    "red_points = np.random.randn(50, 2) - 2*np.ones((50, 2))\n",
    "\n",
    "# Create blue points centered at (2, 2)\n",
    "blue_points = np.random.randn(50, 2) + 2*np.ones((50, 2))\n",
    "\n",
    "# Plot them\n",
    "plt.scatter(red_points[:,0], red_points[:,1], color='red')\n",
    "plt.scatter(blue_points[:,0], blue_points[:,1], color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the red points are centered at $(-2, -2)$ and the blue points are centered at $(2, 2)$. Now, having seen this data, we can ask ourselves whether there is a way to determine if a point should be red or blue. For example, if someone asks us what the color of the point $(3, 2)$ should be, we'd best respond with blue. Even though this point was not part of the data we have seen, we can infer this since it is located in the blue region of the space.\n",
    "\n",
    "But what is the general rule to determine if a point is more likely to be blue than red? Apparently, we can draw a line $y = -x$ that nicely separates the space into a red region and a blue region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11af22390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a line y = -x\n",
    "x_axis = np.linspace(-4, 4, 100)\n",
    "y_axis = -x_axis\n",
    "plt.plot(x_axis, y_axis)\n",
    "\n",
    "# Add the red and blue points\n",
    "plt.scatter(red_points[:,0], red_points[:,1], color='red')\n",
    "plt.scatter(blue_points[:,0], blue_points[:,1], color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implicitly represent this line using a <b>weight vector</b> $w$ and a <b>bias</b> $b$. The line then corresponds to the set of points $x$ where\n",
    "\n",
    "$$w^T x + b = 0.$$\n",
    "\n",
    "In the case above, we have $w = (1, 1)^T$ and $b = 0$. Now, in order to test whether the point is blue or red, we just have to check whether it is above or below the line. This can be achieved by checking the sign of $w^T x + b$. If it is positive, then $x$ is above the line. If it is negative, then $x$ is below the line. Let's perform this test for our example point $(3, 2)^T$:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 1\n",
    "\\end{pmatrix}\n",
    "\\cdot \\begin{pmatrix}\n",
    "3 \\\\\n",
    "2\n",
    "\\end{pmatrix} = 5\n",
    "$$\n",
    "\n",
    "Since 5 > 0, we know that the point is above the line and, therefore, should be classified as blue.\n",
    "\n",
    "## Perceptron definition\n",
    "\n",
    "In general terms, a <b>classifier</b> is a function $\\hat{c} : \\mathbb{R}^d \\rightarrow \\{1, 2, ..., C\\}$ that maps a point onto one of $C$ classes. A <b>binary classifier</b> is a classifier where $C = 2$, i.e. we have two classes. A <b>perceptron</b> with weight $w \\in \\mathbb{R}^d$ and bias $b \\in \\mathbb{R}^d$ is a binary classifier where\n",
    "\n",
    "$$\n",
    "  \\hat{c}(x) = \n",
    "  \\begin{cases}\n",
    "    1, & \\text{if } w^T x + b \\geq 0 \\\\\n",
    "    2, & \\text{if } w^T x + b < 0\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "$\\hat{c}$ partitions $\\mathbb{R}^d$ into two half-spaces, each corresponding to one of the two classes. In the 2-dimensional example above, the partitioning is along a line. In general, the partitioning is along a $d-1$ dimensional hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From classes to probabilities\n",
    "\n",
    "Depending on the application, we may be interested not only in determining the most likely class of a point, but also the probability with which it belongs to that class. Note that the higher the value of $w^T x + b$, the higher is its distance to the separating line and, therefore, the higher is our confidence that it belongs to the blue class. But this value can be arbitrarily high. In order to turn this value into a probability, we need to \"squash\" the values to lie between 0 and 1. One way to do this is by applying the <b>sigmoid</b> function $\\sigma$:\n",
    "\n",
    "$$p(\\hat{c}(x) = 1 \\mid x) = \\sigma(w^T x + b)$$ where $$\\sigma(a) = \\frac{1}{1 + e^{-a}}$$\n",
    "\n",
    "Let's take a look at what the sigmoid function looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCKRyFxGJQCp3EZEIpHIXEYlA/x9y4UxnLz/xSgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11afc9978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an interval from -5 to 5 in steps of 0.01\n",
    "a = np.arange(-5, 5, 0.01)\n",
    "\n",
    "# Compute corresponding sigmoid function values\n",
    "s = 1 / (1 + np.exp(-a))\n",
    "\n",
    "# Plot them\n",
    "plt.plot(a, s)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the sigmoid function assigns a probability of 0.5 to values where $w^T x + b = 0$ (i.e. points on the line) and asymptotes towards 1 the higher the value of $w^T x + b$ becomes, and towards 0 the lower it becomes, which is exactly what we want.\n",
    "\n",
    "Let's now define the sigmoid function as an operation, since we'll need it later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class sigmoid(Operation):\n",
    "    \"\"\"Returns the sigmoid of x element-wise.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, a):\n",
    "        \"\"\"Construct sigmoid\n",
    "        \n",
    "        Args:\n",
    "          a: Input node\n",
    "        \"\"\"\n",
    "        super().__init__([a])\n",
    "\n",
    "    def compute(self, a_value):\n",
    "        \"\"\"Compute the output of the sigmoid operation\n",
    "        \n",
    "        Args:\n",
    "          a_value: Input value\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-a_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire computational graph of the perceptron now looks as follows:\n",
    "\n",
    "<img src='perceptron.png?1' style='height: 150px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Using what we have learned, we can now build a perceptron for the red/blue example in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a new graph\n",
    "Graph().as_default()\n",
    "\n",
    "x = placeholder()\n",
    "w = Variable([1, 1])\n",
    "b = Variable(0)\n",
    "p = sigmoid( add(matmul(w, x), b) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this perceptron to compute the probability that $(3, 2)^T$ is a blue point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.993307149076\n"
     ]
    }
   ],
   "source": [
    "session = Session()\n",
    "print(session.run(p, {\n",
    "    x: [3, 2]\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class perceptron\n",
    "\n",
    "So far, we have used the perceptron as a binary classifier, telling us the probability $p$ that a point $x$ belongs to one of two classes. The probability of $x$ belonging to the respective other class is then given by $1-p$. Generally, however, we have more than two classes. For example, when classifying an image, there may be numerous output classes (dog, chair, human, house, ...). We can extend the perceptron to compute multiple output probabilities.\n",
    "\n",
    "Let $C$ denote the number of output classes. Instead of a weight vector $w$, we introduce a weight matrix $W \\in \\mathbb{R}^{d \\times C}$. Each column of the weight matrix contains the weights of a separate linear classifier - one for each class. Instead of the dot product $w^T x$, we compute $x \\, W$, which returns a vector in $\\mathbb{R}^C$, each of whose entries can be seen as the output of the dot product for a different column of the weight matrix. To this, we add a bias vector $b \\in \\mathbb{R}^m$, containing a distinct bias for each output class. This then yields a vector in $\\mathbb{R}^C$ containing the probabilities for each of the $C$ classes.\n",
    "\n",
    "While this procedure may seem complicated, the matrix multiplication actually just performs multiple linear classifications in parallel, one for each of the $C$ classes - each one with its own separating line, given by a weight vector (one column of $W$) and a bias (one entry of $b$).\n",
    "\n",
    "<img src='perceptron2.png?1' style='height: 150px;'>\n",
    "\n",
    "### Softmax\n",
    "\n",
    "While the original perceptron yielded a single scalar value that we squashed through a sigmoid to obtain a probability between 0 and 1, the multi-class perceptron yields a vector $a \\in \\mathbb{R}^m$. The higher the i-th entry of $a$, the higher is our confidence that the input point belongs to the i-th class. We would like to turn $a$ into a vector of probabilities, such that the probability for every class lies between 0 and 1 and the probabilities for all classes sum up to 1.\n",
    "\n",
    "A common way to do this is to use the <b>softmax function</b>, which is a generalization of the sigmoid to multiple output classes:\n",
    "\n",
    "$$\n",
    "\\sigma(a)_i = \\frac{e^{a_i}}{\\sum_{j = 1}^C e^{a_j}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class softmax(Operation):\n",
    "    \"\"\"Returns the softmax of a.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, a):\n",
    "        \"\"\"Construct softmax\n",
    "        \n",
    "        Args:\n",
    "          a: Input node\n",
    "        \"\"\"\n",
    "        super().__init__([a])\n",
    "\n",
    "    def compute(self, a_value):\n",
    "        \"\"\"Compute the output of the softmax operation\n",
    "        \n",
    "        Args:\n",
    "          a_value: Input value\n",
    "        \"\"\"\n",
    "        return np.exp(a_value) / np.sum(np.exp(a_value), axis = 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch computation\n",
    "\n",
    "The matrix form allows us to feed in more than one point at a time. That is, instead of a single point $x$, we could feed in a matrix $X \\in \\mathbb{R}^{N \\times d}$ containing one point per row (i.e. $N$ rows of $d$-dimensional points). We refer to such a matrix as a <b>batch</b>. Instead of $xW$, we compute $XW$. This returns an $N \\times C$ matrix, each of whose rows contains $xW$ for one point $x$. To each row, we add a bias vector $b$, which is now an $1 \\times m$ row vector. The whole procedure thus computes a function $f : \\mathbb{R}^{N \\times d} \\rightarrow \\mathbb{R}^{m}$ where $f(X) = \\sigma(XW + b)$. The computational graph looks as follows:\n",
    "\n",
    "<img src='perceptron3.png?2' style='height: 150px;'>\n",
    "\n",
    "### Example\n",
    "\n",
    "Let's now generalize our red/blue perceptron to allow for batch computation and multiple output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a new graph\n",
    "Graph().as_default()\n",
    "\n",
    "X = placeholder()\n",
    "\n",
    "# Create a weight matrix for 2 output classes:\n",
    "# One with a weight vector (1, 1) for blue and one with a weight vector (-1, -1) for red\n",
    "W = Variable([\n",
    "    [1, -1],\n",
    "    [1, -1]\n",
    "])\n",
    "b = Variable([0, 0])\n",
    "p = softmax( add(matmul(X, W), b) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.99999755e-01   2.44965755e-07]\n",
      " [  9.99916451e-01   8.35491458e-05]\n",
      " [  9.99110095e-01   8.89905403e-04]\n",
      " [  9.94318073e-01   5.68192656e-03]\n",
      " [  9.99775215e-01   2.24785117e-04]\n",
      " [  9.97613720e-01   2.38628020e-03]\n",
      " [  9.99997742e-01   2.25787019e-06]\n",
      " [  9.99985996e-01   1.40037056e-05]\n",
      " [  9.99539500e-01   4.60499987e-04]\n",
      " [  9.99876553e-01   1.23447085e-04]]\n"
     ]
    }
   ],
   "source": [
    "# Create a session and run the perceptron on our blue/red points\n",
    "session = Session()\n",
    "output_probabilities = session.run(p, {\n",
    "    X: np.concatenate((blue_points, red_points))\n",
    "})\n",
    "\n",
    "# Print the first 10 lines, corresponding to the probabilities of the first 10 points\n",
    "print(output_probabilities[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the first 10 points in our data are all blue, the perceptron outputs high probabilities for blue (left column) and low probabilities for red (right column), as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training criterion\n",
    "\n",
    "Great, so now we are able to classify points using a linear classifier and compute the probability that the point belongs to a certain class, provided that we know the appropriate parameters for the weight matrix $W$ and bias $b$. The natural question that arises is how to come up with appropriate values for these. In the red/blue example, we just looked at the training points and guessed a line that nicely separated the training points. But generally we do not want to specify the separating line by hand. Rather, we just want to supply the training points to the computer and let it come up with a good separating line on its own. But how do we judge whether a separating line is good or bad?\n",
    "\n",
    "## The misclassification rate\n",
    "\n",
    "Ideally, we want to find a line that makes as few errors as possible. For every point $x$ and class $c(x)$ drawn from the true but unknown data-generating distribution $p_\\text{data}(x, c(x))$, we want to minimize the probability that our perceptron classifies it incorrectly - the <b>probability of misclassification</b>:\n",
    "\n",
    "$$\\underset{W, b}{\\operatorname{argmin}} p(\\hat{c}(x) \\neq c(x) \\mid x, c(x) \\, \\tilde{} \\, p_\\text{data} )$$\n",
    "\n",
    "Generally, we do not know the data-generating distribution $p_\\text{data}$, so it is impossible to compute the exact probability of misclassification. Instead, we are given a finite list of $N$ <b>training points</b> consisting of the values of $x$ with their corresponding classes. In the following, we represent the list of training points as a matrix $X \\in \\mathbb{R}^{N \\times d}$ where each row corresponds to one training point and each column to one dimension of the input space. Moreover, we represent the true classes as a matrix $c \\in \\mathbb{R}^{N \\times C}$ where $c_{i, j} = 1$ if the $i$-th training sample has class $j$. Similarly, we represent the predicted classes as a matrix $\\hat{c} \\in \\mathbb{R}^{N \\times C}$ where $\\hat{c}_{i, j} = 1$ if the $i$-th training sample has a predicted class $j$. Finally, we represent the output probabilities of our model as a matrix $p \\in \\mathbb{R}^{N \\times C}$ where $p_{i, j}$ contains the probability that the $i$-th training sample belongs to the j-th class.\n",
    "\n",
    "We could use the training data to find a classifier that minimizes the <b>misclassification rate</b> on the training samples:\n",
    "\n",
    "$$\n",
    "\\underset{W, b}{\\operatorname{argmin}} \\frac{1}{N} \\sum_{i = 1}^N I(\\hat{c}_i \\neq c_i)\n",
    "$$\n",
    "\n",
    "However, it turns out that finding a linear classifier that minimizes the misclassification rate is an intractable problem, i.e. its computational complexity is exponential in the number of input dimensions, rendering it unpractical. Moreover, even if we have found a classifier that minimizes the misclassification rate on the training samples, it might be possible to make the classifier more robust to unseen samples by pushing the classes further apart, even if this does not reduce the misclassification rate on the training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum likelihood estimation\n",
    "\n",
    "An alternative is to use <a href=\"https://en.wikipedia.org/wiki/Maximum_likelihood_estimation\">maximum likelihood estimation</a>, where we try to find the parameters that maximize the probability of the training data:\n",
    "\n",
    "<div class=\"left_equation\" style=\"margin-top: 20px; padding-left: 50px;\">\n",
    "\\begin{align}\n",
    " \\underset{W, b}{\\operatorname{argmax}} p(\\hat{c} = c) \\\\\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "= \\underset{W, b}{\\operatorname{argmax}} \\prod_{i=1}^N p(\\hat{c}_i = c_i) \\\\\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "= \\underset{W, b}{\\operatorname{argmax}} \\prod_{i=1}^N \\prod_{j=1}^C  p_{i, j}^{I(c_i = j)} \\\\\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "= \\underset{W, b}{\\operatorname{argmax}} \\prod_{i=1}^N \\prod_{j=1}^C  p_{i, j}^{c_{i, j}} \\\\\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "= \\underset{W, b}{\\operatorname{argmax}} log \\prod_{i=1}^N \\prod_{j=1}^C  p_{i, j}^{c_{i, j}} \\\\\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "= \\underset{W, b}{\\operatorname{argmax}} \\sum_{i=1}^N \\sum_{j=1}^C c_{i, j} \\cdot log \\, p_{i, j} \\\\\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "= \\underset{W, b}{\\operatorname{argmin}} - \\sum_{i=1}^N \\sum_{j=1}^C c_{i, j} \\cdot log \\, p_{i, j} \\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "= \\underset{W, b}{\\operatorname{argmin}} J\n",
    "\\end{align}\n",
    "\n",
    "</div>\n",
    "\n",
    "We refer to $J = - \\sum_{i=1}^N \\sum_{j=1}^C c_{i, j} \\cdot log \\, p_{i, j}$ as the <b>cross-entropy loss</b>. We want to minimize $J$.\n",
    "\n",
    "We can view $J$ as yet another operation in our computational graph that takes the input data $X$, the true classes $c$ and our predicted probabilities $p$ (which are the output of the $\\sigma$ operation) as input and computes a real number designating the loss:\n",
    "\n",
    "<img src=\"loss_graph.png\" style=\"height: 200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an operation that computes $J$\n",
    "\n",
    "We can build up $J$ from various more primitive operations. Using the element-wise matrix multiplication $\\odot$, we can rewrite $J$ as follows:\n",
    "\n",
    "$$\n",
    "- \\sum_{i=1}^N \\sum_{j=1}^C (c \\odot log \\, p)_{i, j}\n",
    "$$\n",
    "\n",
    "Going from the inside out, we can see that we need to implement the following operations:\n",
    "- $log$: The element-wise logarithm of a matrix or vector\n",
    "- $\\odot$: The element-wise product of two matrices\n",
    "- $\\sum_{j=1}^C$: Sum over the columns of a matrix\n",
    "- $\\sum_{i=1}^N$: Sum over the rows of a matrix\n",
    "- $-$: Taking the negative\n",
    "\n",
    "Let's implement these operations.\n",
    "\n",
    "### log\n",
    "This computes the element-wise logarithm of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class log(Operation):\n",
    "    \"\"\"Computes the natural logarithm of x element-wise.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, x):\n",
    "        \"\"\"Construct log\n",
    "        \n",
    "        Args:\n",
    "          x: Input node\n",
    "        \"\"\"\n",
    "        super().__init__([x])\n",
    "\n",
    "    def compute(self, x_value):\n",
    "        \"\"\"Compute the output of the log operation\n",
    "        \n",
    "        Args:\n",
    "          x_value: Input value\n",
    "        \"\"\"\n",
    "        return np.log(x_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multiply / $\\odot$\n",
    "This computes the element-wise product of two tensors of the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class multiply(Operation):\n",
    "    \"\"\"Returns x * y element-wise.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        \"\"\"Construct multiply\n",
    "        \n",
    "        Args:\n",
    "          x: First multiplicand node\n",
    "          y: Second multiplicand node\n",
    "        \"\"\"\n",
    "        super().__init__([x, y])\n",
    "\n",
    "    def compute(self, x_value, y_value):\n",
    "        \"\"\"Compute the output of the multiply operation\n",
    "        \n",
    "        Args:\n",
    "          x_value: First multiplicand value\n",
    "          y_value: Second multiplicand value\n",
    "        \"\"\"\n",
    "        return x_value * y_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce_sum\n",
    "\n",
    "We'll implement the summation over rows, columns, etc. in a single operation where we specify an `axis`. This way, we can use the same method for all types of summations. For example, `axis = 0` sums over the rows, `axis = 1` sums over the columns, etc. This is exactly what `numpy.sum` does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class reduce_sum(Operation):\n",
    "    \"\"\"Computes the sum of elements across dimensions of a tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, A, axis = None):\n",
    "        \"\"\"Construct reduce_sum\n",
    "        \n",
    "        Args:\n",
    "          A: The tensor to reduce.\n",
    "          axis: The dimensions to reduce. If `None` (the default), reduces all dimensions.\n",
    "        \"\"\"\n",
    "        super().__init__([A])\n",
    "        self.axis = axis\n",
    "\n",
    "    def compute(self, A_value):\n",
    "        \"\"\"Compute the output of the reduce_sum operation\n",
    "        \n",
    "        Args:\n",
    "          A_value: Input tensor value\n",
    "        \"\"\"\n",
    "        return np.sum(A_value, self.axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### negative\n",
    "\n",
    "This computes the element-wise negative of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class negative(Operation):\n",
    "    \"\"\"Computes the negative of x element-wise.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, x):\n",
    "        \"\"\"Construct negative\n",
    "        \n",
    "        Args:\n",
    "          x: Input node\n",
    "        \"\"\"\n",
    "        super().__init__([x])\n",
    "\n",
    "    def compute(self, x_value):\n",
    "        \"\"\"Compute the output of the negative operation\n",
    "        \n",
    "        Args:\n",
    "          x_value: Input value\n",
    "        \"\"\"\n",
    "        return -x_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "Using these operations, we can now compute $J = - \\sum_{i=1}^N \\sum_{j=1}^C (c \\odot log \\, p)_{i, j}\n",
    "$ as follows:\n",
    "\n",
    "    J = negative(reduce_sum(reduce_sum(multiply(c, log(p)), axis=1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Let's now compute the loss of our red/blue perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.93977651159\n"
     ]
    }
   ],
   "source": [
    "# Create a new graph\n",
    "Graph().as_default()\n",
    "\n",
    "X = placeholder()\n",
    "c = placeholder()\n",
    "\n",
    "W = Variable([\n",
    "    [1, -1],\n",
    "    [1, -1]\n",
    "])\n",
    "b = Variable([0, 0])\n",
    "p = softmax( add(matmul(X, W), b) )\n",
    "\n",
    "# Cross-entropy loss\n",
    "J = negative(reduce_sum(reduce_sum(multiply(c, log(p)), axis=1)))\n",
    "\n",
    "session = Session()\n",
    "print(session.run(J, {\n",
    "    X: np.concatenate((blue_points, red_points)),\n",
    "    c:\n",
    "        [[1, 0]] * len(blue_points)\n",
    "        + [[0, 1]] * len(red_points)\n",
    "    \n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "\n",
    "Generally, if we want to find the minimum of a function, we set the derivative to zero and solve for the parameters. It turns out, however, that it is impossible to obtain a closed-form solution for $W$ and $b$. Instead, we iteratively search for a minimum using a method called <b>gradient descent</b>.\n",
    "\n",
    "As a visual analogy, imagine yourself standing on a mountain and trying to find the way down. At every step, you walk into the steepest direction, since this direction is the most promising to lead you towards the bottom.\n",
    "\n",
    "<img src=\"gradient_descent.png\" style=\"width: 400px;\">\n",
    "\n",
    "If taking steep steps seems a little dangerous to you, imagine that you are a mountain goat (which are <a href=\"https://www.youtube.com/watch?v=hZWxlxrIWus\">amazing rock climbers</a>).\n",
    "\n",
    "Gradient descent operates in a similar way when trying to find the minimum of a function: It starts at a random location in parameter space and then iteratively reduces the error $J$ until it reaches a local minimum. At each step of the iteration, it determines the direction of steepest descent and takes a step along that direction. This process is depicted for the 1-dimensional case in the following image.\n",
    "\n",
    "<img src=\"gradient_descent_2.png?123\" style=\"width: 400px;\">\n",
    "\n",
    "As you might remember, the direction of steepest ascent of a function at a certain point is given by the gradient at that point. Therefore, the direction of steepest descent is given by the negative of the gradient. So now we have a rough idea how to minimize $J$:\n",
    "1. Start with random values for $W$ and $b$\n",
    "2. Compute the gradients of $J$ with respect to $W$ and $b$\n",
    "3. Take a small step along the direction of the negative gradient\n",
    "4. Go back to 2\n",
    "\n",
    "Let's implement an operation that minimizes the value of a node using gradient descent. We require the user to specify the magnitude of the step along the gradient as a parameter called `learning_rate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "\n",
    "class GradientDescentOptimizer:\n",
    "    \n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def minimize(self, loss):\n",
    "        learning_rate = self.learning_rate\n",
    "        class MinimizationOperation(Operation):\n",
    "            def compute(self):\n",
    "                # Compute gradients\n",
    "                grad_table = compute_gradients(loss)\n",
    "                \n",
    "                # Iterate all variables\n",
    "                for node in grad_table:\n",
    "                    if type(node) == Variable:\n",
    "                        # Retrieve gradient for this variable\n",
    "                        grad = grad_table[node]\n",
    "                                                \n",
    "                        # Take a step along the direction of the negative gradient\n",
    "                        node.value -= learning_rate * grad\n",
    "                \n",
    "                \n",
    "        return MinimizationOperation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following image depicts an example iteration of gradient descent. We start out with a random separating line (marked as 1), take a step, arrive at a slightly better line (marked as 2), take another step, and another step, and so on until we arrive at a good separating line.\n",
    "\n",
    "<img src=\"gradient_descent_3.png\" style=\"height: 250px;\">\n",
    "\n",
    "# Backpropagation\n",
    "\n",
    "In our implementation of gradient descent, we have used a function `compute_gradient(loss)` that computes the gradient of a $loss$ operation in our computational graph with respect to the output of every other node $n$ (i.e. the direction of change for $n$ along which the loss increases the most). We now need to figure out how to compute gradients.\n",
    "\n",
    "Consider the following computational graph:\n",
    "\n",
    "<img src=\"abcde.png\" style=\"height: 70px;\">\n",
    "\n",
    "By the <a href=\"https://en.wikipedia.org/wiki/Chain_rule\">chain rule</a>, we have\n",
    "\n",
    "$$\\frac{\\partial e}{\\partial a} = \\frac{\\partial e}{\\partial b} \\cdot \\frac{\\partial b}{\\partial a} = \\frac{\\partial e}{\\partial c} \\cdot \\frac{\\partial c}{\\partial b} \\cdot \\frac{\\partial b}{\\partial a} = \\frac{\\partial e}{\\partial d} \\cdot \\frac{\\partial d}{\\partial c} \\cdot \\frac{\\partial c}{\\partial b} \\cdot \\frac{\\partial b}{\\partial a}$$\n",
    "\n",
    "As we can see, in order to compute the gradient of $e$ with respect to $a$, we can start at $e$ an go backwards towards $a$, computing the gradient of every node's output with respect to its input along the way until we reach $a$. Then, we multiply them all together.\n",
    "\n",
    "Now consider the following scenario:\n",
    "\n",
    "<img src=\"abcde2.png\" style=\"height: 160px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, $a$ contributes to $e$ along two paths: The path $a$, $b$, $d$, $e$ and the path $a$, $b$, $c$, $e$. Hence, the <a href=\"https://en.wikipedia.org/wiki/Total_derivative\">total derivative</a> of $e$ with respect to $a$ is given by:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial e}{\\partial a}\n",
    "= \\frac{\\partial e}{\\partial d} \\cdot \\frac{\\partial d}{\\partial a}\n",
    "= \\frac{\\partial e}{\\partial d} \\cdot \\left( \\frac{\\partial d}{\\partial b} \\cdot \\frac{\\partial b}{\\partial a} + \\frac{\\partial d}{\\partial c} \\cdot \\frac{\\partial c}{\\partial a} \\right)\n",
    "= \\frac{\\partial e}{\\partial d} \\cdot \\frac{\\partial d}{\\partial b} \\cdot \\frac{\\partial b}{\\partial a} + \\frac{\\partial e}{\\partial d} \\cdot \\frac{\\partial d}{\\partial c} \\cdot \\frac{\\partial c}{\\partial a}\n",
    "$$\n",
    "\n",
    "This gives as an intuition for the general algorithm that computes the gradient of the loss with respect to another node: We perform a backwards <a href=\"https://en.wikipedia.org/wiki/Breadth-first_search\">breadth-first search</a> starting from the loss node. At each node $n$ that we visit, we do the following for each of its consumers:\n",
    "- retrieve the gradient $G$ of the loss with respect to the output of the consumer\n",
    "- multiply $G$ by the gradient of the consumer's output with respect to $n$'s output\n",
    "\n",
    "And then we sum those gradients over all consumers.\n",
    "\n",
    "As a prerequisite to implementing backpropagation, we need to specify a function for each operation that computes the gradients with respect to the inputs of that operation, given the gradients with respect to the output. Let's define a decorator `@RegisterGradient(operation_name)` for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A dictionary that will map operations to gradient functions\n",
    "_gradient_registry = {}\n",
    "            \n",
    "class RegisterGradient:\n",
    "    \"\"\"A decorator for registering the gradient function for an op type.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, op_type):\n",
    "        \"\"\"Creates a new decorator with `op_type` as the Operation type.\n",
    "        Args:\n",
    "          op_type: The name of an operation\n",
    "        \"\"\"\n",
    "        self._op_type = eval(op_type)\n",
    "\n",
    "    def __call__(self, f):\n",
    "        \"\"\"Registers the function `f` as gradient function for `op_type`.\"\"\"\n",
    "        _gradient_registry[self._op_type] = f\n",
    "        return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now assume that our `_gradient_registry` dictionary is already filled with gradient computation functions for all of our operations. We can now implement backpropagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "\n",
    "def compute_gradients(loss):\n",
    "\n",
    "    # grad_table[node] will contain the gradient of the loss w.r.t. the node's output\n",
    "    grad_table = {}\n",
    "\n",
    "    # The gradient of the loss with respect to the loss is just 1\n",
    "    grad_table[loss] = 1\n",
    "\n",
    "    # Perform a breadth-first search, backwards from the loss\n",
    "    visited = set()\n",
    "    queue = Queue()\n",
    "    visited.add(loss)\n",
    "    queue.put(loss)\n",
    "\n",
    "    while not queue.empty():\n",
    "        node = queue.get()\n",
    "\n",
    "        # If this node is not the loss\n",
    "        if node != loss:\n",
    "            #\n",
    "            # Compute the gradient of the loss with respect to this node's output\n",
    "            #\n",
    "            grad_table[node] = 0\n",
    "\n",
    "            # Iterate all consumers\n",
    "            for consumer in node.consumers:\n",
    "\n",
    "                # Retrieve the gradient of the loss w.r.t. consumer's output\n",
    "                lossgrad_wrt_consumer_output = grad_table[consumer]\n",
    "\n",
    "                # Retrieve the function which computes gradients with respect to\n",
    "                # consumer's inputs given gradients with respect to consumer's output.\n",
    "                consumer_op_type = consumer.__class__\n",
    "                bprop = _gradient_registry[consumer_op_type]\n",
    "\n",
    "                # Get the gradient of the loss with respect to all of consumer's inputs\n",
    "                lossgrads_wrt_consumer_inputs = bprop(consumer, lossgrad_wrt_consumer_output)\n",
    "                \n",
    "                if len(consumer.input_nodes) == 1:\n",
    "                    # If there is a single input node to the consumer, lossgrads_wrt_consumer_inputs is a scalar\n",
    "                    grad_table[node] += lossgrads_wrt_consumer_inputs\n",
    "                    \n",
    "                else:\n",
    "                    # Otherwise, lossgrads_wrt_consumer_inputs is an array of gradients for each input node\n",
    "                \n",
    "                    # Retrieve the index of node in consumer's inputs\n",
    "                    node_index_in_consumer_inputs = consumer.input_nodes.index(node)\n",
    "\n",
    "                    # Get the gradient of the loss with respect to node\n",
    "                    lossgrad_wrt_node = lossgrads_wrt_consumer_inputs[node_index_in_consumer_inputs]\n",
    "\n",
    "                    # Add to total gradient\n",
    "                    grad_table[node] += lossgrad_wrt_node\n",
    "\n",
    "        #\n",
    "        # Append each input node to the queue\n",
    "        #\n",
    "        if hasattr(node, \"input_nodes\"):\n",
    "            for input_node in node.input_nodes:\n",
    "                if not input_node in visited:\n",
    "                    visited.add(input_node)\n",
    "                    queue.put(input_node)\n",
    "            \n",
    "    # Return gradients for each visited node\n",
    "    return grad_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient of each operation\n",
    "\n",
    "For each of our operations, we now need to define a function that turns a gradient of the loss with respect to the operation's output into a list of gradients of the loss with respect to each of the operation's inputs. Computing a gradient with respect to a matrix can be somewhat tedious. Therefore, the details have been omitted and I just present the results. You may skip this section and still understand the overall picture.\n",
    "\n",
    "If you want to comprehend how to arrive at the results, the general approach is as follows:\n",
    "- Find the partial derivative of each output value with respect to each input value (this can be a tensor of a rank greater than 2, i.e. neither scalar nor vector nor matrix, involving a lot of summations)\n",
    "- Compute the gradient of the loss with respect to the node's inputs given a gradient with respect to the node's output by applying the chain rule. This is now a tensor of the same shape as the input tensor, so if the input is a matrix, the result is also a matrix\n",
    "- Rewrite this result as a sequence of matrix operations in order to compute it efficiently. This step can be somewhat tricky.\n",
    "\n",
    "### Gradient for `add`\n",
    "Given a gradient $G$ with respect to $a + b$, the gradient with respect to $a$ is given by $G$ and the gradient with respect to $b$ is also given by $G$, provided that $a$ and $b$ are of the same shape. If $a$ and $b$ are of different shapes, e.g. one matrix $a$ with 100 rows and one row vector $b$, we assume that $b$ is added to each row of $a$. In this case, the gradient computation is a little more involved, but I will not spell out the details here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@RegisterGradient(\"add\")\n",
    "def _add_gradient(op, grad):\n",
    "    \"\"\"Computes the gradients for `add`.\n",
    "\n",
    "    Args:\n",
    "      op: The `add` `Operation` that we are differentiating\n",
    "      grad: Gradient with respect to the output of the `add` op.\n",
    "\n",
    "    Returns:\n",
    "      Gradients with respect to the input of `add`.\n",
    "    \"\"\"\n",
    "    a = op.inputs[0]\n",
    "    b = op.inputs[1]\n",
    "    \n",
    "    grad_wrt_a = grad\n",
    "    while np.ndim(grad_wrt_a) > len(a.shape):\n",
    "        grad_wrt_a = np.sum(grad_wrt_a, axis=0)\n",
    "    for axis, size in enumerate(a.shape):\n",
    "        if size == 1:\n",
    "            grad_wrt_a = np.sum(grad_wrt_a, axis=axis, keepdims=True)\n",
    "    \n",
    "    grad_wrt_b = grad\n",
    "    while np.ndim(grad_wrt_b) > len(b.shape):\n",
    "        grad_wrt_b = np.sum(grad_wrt_b, axis=0)\n",
    "    for axis, size in enumerate(b.shape):\n",
    "        if size == 1:\n",
    "            grad_wrt_b = np.sum(grad_wrt_b, axis=axis, keepdims=True)\n",
    "    \n",
    "    return [grad_wrt_a, grad_wrt_b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient for `matmul`\n",
    "\n",
    "Given a gradient $G$ with respect to $AB$, the gradient with respect to $A$ is given by $GB^T$ and the gradient with respect to $B$ is given by $A^TG$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@RegisterGradient(\"matmul\")\n",
    "def _matmul_gradient(op, grad):\n",
    "    \"\"\"Computes the gradients for `matmul`.\n",
    "\n",
    "    Args:\n",
    "      op: The `matmul` `Operation` that we are differentiating\n",
    "      grad: Gradient with respect to the output of the `matmul` op.\n",
    "\n",
    "    Returns:\n",
    "      Gradients with respect to the input of `matmul`.\n",
    "    \"\"\"\n",
    "\n",
    "    A = op.inputs[0]\n",
    "    B = op.inputs[1]\n",
    "\n",
    "    return [grad.dot(B.T), A.T.dot(grad)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient for `sigmoid`\n",
    "\n",
    "Given a gradient $G$ with respect to $\\sigma(a)$, the gradient with respect to $a$ is given by $G \\cdot \\sigma(a) \\cdot \\sigma(1-a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@RegisterGradient(\"sigmoid\")\n",
    "def _sigmoid_gradient(op, grad):\n",
    "    \"\"\"Computes the gradients for `sigmoid`.\n",
    "\n",
    "    Args:\n",
    "      op: The `sigmoid` `Operation` that we are differentiating\n",
    "      grad: Gradient with respect to the output of the `sigmoid` op.\n",
    "\n",
    "    Returns:\n",
    "      Gradients with respect to the input of `sigmoid`.\n",
    "    \"\"\"\n",
    "    \n",
    "    sigmoid = op.output\n",
    "\n",
    "    return grad * sigmoid * (1-sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient for `softmax`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@RegisterGradient(\"softmax\")\n",
    "def _softmax_gradient(op, grad):\n",
    "    \"\"\"Computes the gradients for `softmax`.\n",
    "\n",
    "    Args:\n",
    "      op: The `softmax` `Operation` that we are differentiating\n",
    "      grad: Gradient with respect to the output of the `softmax` op.\n",
    "\n",
    "    Returns:\n",
    "      Gradients with respect to the input of `softmax`.\n",
    "    \"\"\"\n",
    "    \n",
    "    softmax = op.output\n",
    "    return (grad - np.reshape(\n",
    "        np.sum(grad * softmax, 1),\n",
    "        [-1, 1]\n",
    "    )) * softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient for `log`\n",
    "\n",
    "Given a gradient $G$ with respect to $log(x)$, the gradient with respect to $x$ is given by $\\frac{G}{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@RegisterGradient(\"log\")\n",
    "def _log_gradient(op, grad):\n",
    "    \"\"\"Computes the gradients for `log`.\n",
    "\n",
    "    Args:\n",
    "      op: The `log` `Operation` that we are differentiating\n",
    "      grad: Gradient with respect to the output of the `log` op.\n",
    "\n",
    "    Returns:\n",
    "      Gradients with respect to the input of `log`.\n",
    "    \"\"\"\n",
    "    x = op.inputs[0]\n",
    "    return grad/x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient for `multiply`\n",
    "\n",
    "Given a gradient $G$ with respect to $A \\odot B$, the gradient with respect to $A$ is given by $G \\odot B$ and the gradient with respect to $B$ is given by $G \\odot A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@RegisterGradient(\"multiply\")\n",
    "def _multiply_gradient(op, grad):\n",
    "    \"\"\"Computes the gradients for `multiply`.\n",
    "\n",
    "    Args:\n",
    "      op: The `multiply` `Operation` that we are differentiating\n",
    "      grad: Gradient with respect to the output of the `multiply` op.\n",
    "\n",
    "    Returns:\n",
    "      Gradients with respect to the input of `multiply`.\n",
    "    \"\"\"\n",
    "\n",
    "    A = op.inputs[0]\n",
    "    B = op.inputs[1]\n",
    "\n",
    "    return [grad * B, grad * A]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient for `reduce_sum`\n",
    "\n",
    "Given a gradient $G$ with respect to the output of `reduce_sum`, the gradient with respect to the input $A$ is given by repeating $G$ along the specified axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@RegisterGradient(\"reduce_sum\")\n",
    "def _reduce_sum_gradient(op, grad):\n",
    "    \"\"\"Computes the gradients for `reduce_sum`.\n",
    "\n",
    "    Args:\n",
    "      op: The `reduce_sum` `Operation` that we are differentiating\n",
    "      grad: Gradient with respect to the output of the `reduce_sum` op.\n",
    "\n",
    "    Returns:\n",
    "      Gradients with respect to the input of `reduce_sum`.\n",
    "    \"\"\"\n",
    "    A = op.inputs[0]\n",
    "\n",
    "    output_shape = np.array(A.shape)\n",
    "    output_shape[op.axis] = 1\n",
    "    tile_scaling = A.shape // output_shape\n",
    "    grad = np.reshape(grad, output_shape)\n",
    "    return np.tile(grad, tile_scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient for `negative`\n",
    "\n",
    "Given a gradient $G$ with respect to $-x$, the gradient with respect to $x$ is given by $-G$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@RegisterGradient(\"negative\")\n",
    "def _negative_gradient(op, grad):\n",
    "    \"\"\"Computes the gradients for `negative`.\n",
    "\n",
    "    Args:\n",
    "      op: The `negative` `Operation` that we are differentiating\n",
    "      grad: Gradient with respect to the output of the `negative` op.\n",
    "\n",
    "    Returns:\n",
    "      Gradients with respect to the input of `negative`.\n",
    "    \"\"\"\n",
    "    return -grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Example\n",
    "\n",
    "Let's now test our implementation to determine the optimal weights for our perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0  Loss: 202.782788396\n",
      "Step: 10  Loss: 4.04566479054\n",
      "Step: 20  Loss: 2.69644468305\n",
      "Step: 30  Loss: 2.00506261735\n",
      "Step: 40  Loss: 1.62202006027\n",
      "Step: 50  Loss: 1.39268559111\n",
      "Step: 60  Loss: 1.24498439759\n",
      "Step: 70  Loss: 1.14348265257\n",
      "Step: 80  Loss: 1.06965484385\n",
      "Step: 90  Loss: 1.01324253829\n",
      "Weight matrix:\n",
      " [[ 1.27496197 -1.77251219]\n",
      " [ 1.11820232 -2.01586474]]\n",
      "Bias:\n",
      " [-0.45274057 -0.39071841]\n"
     ]
    }
   ],
   "source": [
    "# Create a new graph\n",
    "Graph().as_default()\n",
    "\n",
    "X = placeholder()\n",
    "c = placeholder()\n",
    "\n",
    "# Initialize weights randomly\n",
    "W = Variable(np.random.randn(2, 2))\n",
    "b = Variable(np.random.randn(2))\n",
    "\n",
    "# Build perceptron\n",
    "p = softmax( add(matmul(X, W), b) )\n",
    "\n",
    "# Build cross-entropy loss\n",
    "J = negative(reduce_sum(reduce_sum(multiply(c, log(p)), axis=1)))\n",
    "\n",
    "# Build minimization op\n",
    "minimization_op = GradientDescentOptimizer(learning_rate = 0.01).minimize(J)\n",
    "\n",
    "# Build placeholder inputs\n",
    "feed_dict = {\n",
    "    X: np.concatenate((blue_points, red_points)),\n",
    "    c:\n",
    "        [[1, 0]] * len(blue_points)\n",
    "        + [[0, 1]] * len(red_points)\n",
    "    \n",
    "}\n",
    "\n",
    "# Create session\n",
    "session = Session()\n",
    "\n",
    "# Perform 100 gradient descent steps\n",
    "for step in range(100):\n",
    "    J_value = session.run(J, feed_dict)\n",
    "    if step % 10 == 0:\n",
    "        print(\"Step:\", step, \" Loss:\", J_value)\n",
    "    session.run(minimization_op, feed_dict)\n",
    "\n",
    "# Print final result\n",
    "W_value = session.run(W)\n",
    "print(\"Weight matrix:\\n\", W_value)\n",
    "b_value = session.run(b)\n",
    "print(\"Bias:\\n\", b_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we started out with a rather high loss and incrementally reduced it. Let's plot the final line to check that it is a good separator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nw8RORGQz/wdotmyXLfsf+QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ab9d1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a line y = -x\n",
    "x_axis = np.linspace(-4, 4, 100)\n",
    "y_axis = -W_value[0][0]/W_value[1][0] * x_axis - b_value[0]/W_value[1][0]\n",
    "plt.plot(x_axis, y_axis)\n",
    "\n",
    "# Add the red and blue points\n",
    "plt.scatter(red_points[:,0], red_points[:,1], color='red')\n",
    "plt.scatter(blue_points[:,0], blue_points[:,1], color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer perceptrons\n",
    "\n",
    "## Motivation\n",
    "\n",
    "So now we are able to train linear classifiers of arbitrary dimensionality automatically. However, many real-world classes are not linearly separable. This means that there does not exist any line with all the points of the first class on one side of the line and all the points of the other class on the other side. Let's illustrate this with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a43a0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create two clusters of red points centered at (0, 0) and (1, 1), respectively.\n",
    "red_points = np.concatenate((\n",
    "    0.2*np.random.randn(25, 2) + np.array([[0, 0]]*25),\n",
    "    0.2*np.random.randn(25, 2) + np.array([[1, 1]]*25)\n",
    "))\n",
    "\n",
    "# Create two clusters of blue points centered at (0, 1) and (1, 0), respectively.\n",
    "blue_points = np.concatenate((\n",
    "    0.2*np.random.randn(25, 2) + np.array([[0, 1]]*25),\n",
    "    0.2*np.random.randn(25, 2) + np.array([[1, 0]]*25)\n",
    "))\n",
    "\n",
    "# Plot them\n",
    "plt.scatter(red_points[:,0], red_points[:,1], color='red')\n",
    "plt.scatter(blue_points[:,0], blue_points[:,1], color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it is impossible to draw a line that separates the blue points from the red points. Instead, our decision boundary has to have a rather complex shape. This is where multi-layer perceptrons come into play: They allow us to train a decision boundary of a more complex shape than a straight line.\n",
    "\n",
    "## Computational graph\n",
    "\n",
    "As their name suggests, multi-layer perceptrons (MLPs) are composed of multiple perceptrons stacked one after the other in a layer-wise fashion. Let's look at a visualization of the computational graph:\n",
    "\n",
    "<img src=\"mlp.png?2\">\n",
    "\n",
    "As we can see, the input is fed into the first layer, which is a multidimensional perceptron with a weight matrix $W_1$ and bias vector $b_1$. The output of that layer is then fed into second layer, which is again a perceptron with another weight matrix $W_2$ and bias vector $b_2$. This process continues for every of the $L$ layers until we reach the output layer. We refer to the last layer as the <b>output layer</b> and to every other layer as a <b>hidden layer</b>.\n",
    "\n",
    "an MLP with one hidden layers computes the function\n",
    "\n",
    "$$\\sigma(\\sigma(X \\, W_1 + b_1) W_2 + b_2) \\,,$$\n",
    "\n",
    "an MLP with two hidden layers computes the function\n",
    "\n",
    "$$\\sigma(\\sigma(\\sigma(X \\, W_1 + b_1) W_2 + b_2) \\, W_3 \\,,$$\n",
    "\n",
    "and, generally, an MLP with $L-1$ hidden layers computes the function\n",
    "\n",
    "$$\\sigma(\\sigma( \\cdots \\sigma(\\sigma(X \\, W_1 + b_1) W_2 + b_2) \\cdots) \\, W_L + b_L) \\,.$$\n",
    "\n",
    "## Implementation\n",
    "\n",
    "Using the library we have built, we can now easily implement multi-layer perceptrons without further work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0  Loss: 204.794853521\n",
      "Step: 100  Loss: 24.5875895132\n",
      "Step: 200  Loss: 19.3834164328\n",
      "Step: 300  Loss: 18.5417510177\n",
      "Step: 400  Loss: 18.1508174765\n",
      "Step: 500  Loss: 17.9190202357\n",
      "Step: 600  Loss: 17.7638704135\n",
      "Step: 700  Loss: 17.6519581341\n",
      "Step: 800  Loss: 17.5669493256\n",
      "Step: 900  Loss: 17.499848539\n",
      "Hidden layer weight matrix:\n",
      " [[ 3.81177127  7.54753546]\n",
      " [-3.81063897 -8.22017527]]\n",
      "Hidden layer bias:\n",
      " [-3.61945898  5.13708263]\n",
      "Output layer weight matrix:\n",
      " [[ 7.01720731 -5.87228322]\n",
      " [-4.49388731  7.14992648]]\n",
      "Output layer bias:\n",
      " [ 4.16129061 -3.93433668]\n"
     ]
    }
   ],
   "source": [
    "# Create a new graph\n",
    "Graph().as_default()\n",
    "\n",
    "# Create training input placeholder\n",
    "X = placeholder()\n",
    "\n",
    "# Create placeholder for the training classes\n",
    "c = placeholder()\n",
    "\n",
    "# Build a hidden layer\n",
    "W_hidden = Variable(np.random.randn(2, 2))\n",
    "b_hidden = Variable(np.random.randn(2))\n",
    "p_hidden = sigmoid( add(matmul(X, W_hidden), b_hidden) )\n",
    "\n",
    "# Build the output layer\n",
    "W_output = Variable(np.random.randn(2, 2))\n",
    "b_output = Variable(np.random.randn(2))\n",
    "p_output = softmax( add(matmul(p_hidden, W_output), b_output) )\n",
    "\n",
    "# Build cross-entropy loss\n",
    "J = negative(reduce_sum(reduce_sum(multiply(c, log(p_output)), axis=1)))\n",
    "\n",
    "# Build minimization op\n",
    "minimization_op = GradientDescentOptimizer(learning_rate = 0.03).minimize(J)\n",
    "\n",
    "# Build placeholder inputs\n",
    "feed_dict = {\n",
    "    X: np.concatenate((blue_points, red_points)),\n",
    "    c:\n",
    "        [[1, 0]] * len(blue_points)\n",
    "        + [[0, 1]] * len(red_points)\n",
    "    \n",
    "}\n",
    "\n",
    "# Create session\n",
    "session = Session()\n",
    "\n",
    "# Perform 100 gradient descent steps\n",
    "for step in range(1000):\n",
    "    J_value = session.run(J, feed_dict)\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \" Loss:\", J_value)\n",
    "    session.run(minimization_op, feed_dict)\n",
    "\n",
    "# Print final result\n",
    "W_hidden_value = session.run(W_hidden)\n",
    "print(\"Hidden layer weight matrix:\\n\", W_hidden_value)\n",
    "b_hidden_value = session.run(b_hidden)\n",
    "print(\"Hidden layer bias:\\n\", b_hidden_value)\n",
    "W_output_value = session.run(W_output)\n",
    "print(\"Output layer weight matrix:\\n\", W_output_value)\n",
    "b_output_value = session.run(b_output)\n",
    "print(\"Output layer bias:\\n\", b_output_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize the decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11afef240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize classification boundary\n",
    "xs = np.linspace(-2, 2)\n",
    "ys = np.linspace(-2, 2)\n",
    "pred_classes = []\n",
    "for x in xs:\n",
    "    for y in ys:\n",
    "        pred_class = session.run(p_output,\n",
    "                              feed_dict={X: [[x, y]]})[0]\n",
    "        pred_classes.append((x, y, pred_class.argmax()))\n",
    "xs_p, ys_p = [], []\n",
    "xs_n, ys_n = [], []\n",
    "for x, y, c in pred_classes:\n",
    "    if c == 0:\n",
    "        xs_n.append(x)\n",
    "        ys_n.append(y)\n",
    "    else:\n",
    "        xs_p.append(x)\n",
    "        ys_p.append(y)\n",
    "plt.plot(xs_p, ys_p, 'ro', xs_n, ys_n, 'bo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have learned a rather complex decision boundary. If we use more layers, the decision boundary can become arbitrarily complex, allowing us to learn classification patterns that are impossible to spot by a human being, especially in higher dimensions.\n",
    "\n",
    "# Recap\n",
    "\n",
    "Congratulations on making it this far! You have learned the foundations of building neural networks from scratch, and in contrast to most machine learning practitioners, you now know how it all works under the hood and why it is done the way it is done.\n",
    "\n",
    "Let's recap what we have learned. We started out by considering <b>computational graphs</b> in general, and we saw how to build them and how to compute their output. We then moved on to describe <b>perceptrons</b>, which are linear classifiers that assign a probability to each output class by squashing the output of $w^Tx+b$ through a <b>sigmoid</b> (or <b>softmax</b>, in the case of multiple classes). Following that, we saw how to judge how good a classifier is - via a loss function, the <b>cross-entropy loss</b>, the minimization of which is equivalent to <b>maximum likelihood</b>. In the next step, we saw how to minimize the loss via <b>gradient descent</b>: By iteratively stepping into the direction of the negative gradient. We then introduced <b>backpropagation</b> as a means of computing the derivative of the loss with respect to each node by performing a breadth-first search and multiplying according to the chain rule. We used all that we've learned to train a good linear classifier for the red/blue example dataset. Finally, we learned about <b>multi-layer perceptrons</b> as a means of learning non-linear decision boundaries, implemented an MLP with one hidden layer and successfully trained it on a non-linearly-separable dataset.\n",
    "\n",
    "# Next steps\n",
    "\n",
    "You now know all the fundamentals for training arbitrary neural networks. As a next step, you should learn about the following topics (Google is your friend):\n",
    "- The difference between training loss and test loss\n",
    "- Overfitting and underfitting\n",
    "- Regularization and early stopping\n",
    "- Dropout\n",
    "- Convolutional neural networks\n",
    "- Recurrent neural networks\n",
    "- Autoencoders\n",
    "- Deep Generative Models\n",
    "\n",
    "All of these topics are dealt with in the book \"Deep Learning\" by Ian Goodfellow, Yoshua Bengio and Aaron Courville, which I highly recommend everyone to read. A free online version of the book can be found at http://www.deeplearningbook.org/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}